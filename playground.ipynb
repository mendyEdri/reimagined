{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.dev.wixpress.com/artifactory/api/pypi/pypi-repos/simple\n",
      "Requirement already satisfied: pygraphviz in ./.conda/lib/python3.11/site-packages (1.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U --no-cache-dir  \\\n",
    "            --config-settings=\"--global-option=build_ext\" \\\n",
    "            --config-settings=\"--global-option=-I$(brew --prefix graphviz)/include/\" \\\n",
    "            --config-settings=\"--global-option=-L$(brew --prefix graphviz)/lib/\" \\\n",
    "            pygraphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "os.environ['GROQ_API_KEY'] = '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'\n",
    "os.environ['CLAUDE_API_KEY'] = \"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\"\n",
    "os.environ['TOGETHER_API_KEY'] = '2b0eb6b8ee798bb393d55b26678465bb7cd57b1bdc56255da645ae76df2d8c5b'\n",
    "os.environ['TAVILY_API_KEY'] = \"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\"\n",
    "\n",
    "\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage, AIMessage\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain_groq import ChatGroq\n",
    "from groq import Groq\n",
    "\n",
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "# Groq\n",
    "# modelName = \"llama-3.1-8b-instant\"\n",
    "# llm = Groq()\n",
    "\n",
    "# OpenAI\n",
    "modelName = \"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(model=modelName)\n",
    "\n",
    "# MoA\n",
    "# llm = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "# modelName = 'mistralai/Mixtral-8x22B-Instruct-v0.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define state\n",
    "# class GraphState(TypedDict):\n",
    "#     \"\"\"\n",
    "#     Represents the state of our graph.\n",
    "\n",
    "#     Attributes:\n",
    "#         messages: question\n",
    "#         generation: LLM generation\n",
    "#         documents: list of documents\n",
    "#     \"\"\"\n",
    "\n",
    "#     messages: str\n",
    "#     generation: str\n",
    "#     documents: List[str]\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    code: str\n",
    "    docs: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLMs.Invokable import Invokable\n",
    "from utils.files import write_file, write_docs, read_file\n",
    "import random\n",
    "\n",
    "thread_id = 1\n",
    "\n",
    "class Agents:\n",
    "    Planner = \"planner\"\n",
    "    InterfaceWriter = 'interface_code_writer'\n",
    "    CodeWriter = 'code_writer'\n",
    "    Tools = \"tools\"\n",
    "    DocumentationWriter = 'docs_writer'\n",
    "\n",
    "class ReGraph():\n",
    "    def __init__(self):\n",
    "        self.memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.graph\n",
    "\n",
    "    def make_graph(self):\n",
    "        self.model = ChatOpenAI(model='gpt-4o-mini').bind_tools(self.tools)\n",
    "        tool_node = ToolNode(self.tools)\n",
    "        self.graph = StateGraph(AgentState)\n",
    "        self.graph.add_node(Agents.Planner, self.planner_llm)\n",
    "        # self.graph.add_node(Agents.InterfaceWriter, self.)\n",
    "        self.graph.add_node(Agents.CodeWriter, self.code_writer_llm)\n",
    "        self.graph.add_node(Agents.Tools, tool_node)\n",
    "        self.graph.add_node(Agents.DocumentationWriter, self.documentation_writer_llm)\n",
    "        self.add_nodes()\n",
    "\n",
    "        self.graph.add_conditional_edges(Agents.Planner, self.should_continue)\n",
    "\n",
    "        self.graph.add_edge(Agents.CodeWriter, Agents.Planner)\n",
    "        self.graph.add_edge(Agents.Tools, Agents.Planner)\n",
    "        self.graph.add_edge(Agents.DocumentationWriter, Agents.Planner)\n",
    "        self.add_edges()\n",
    "        \n",
    "        self.graph.set_entry_point(Agents.Planner)\n",
    "\n",
    "        self.graph = self.graph.compile(memory)\n",
    "        return self.graph\n",
    "    \n",
    "    def start(self, messages):\n",
    "        print(\"start:\", messages)\n",
    "        return self.graph.invoke(\n",
    "            {\"messages\": messages},\n",
    "            config={\"configurable\": {\"thread_id\": random.randint(1, 10)}}\n",
    "        )\n",
    "\n",
    "    def should_continue(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        last_message = messages[-1]\n",
    "        print(\"*** should continue ***\", last_message)\n",
    "        if last_message.tool_calls:\n",
    "            return Agents.Tools\n",
    "        elif not 'code' in state:\n",
    "            return Agents.CodeWriter\n",
    "        elif not 'docs' in state:\n",
    "            return Agents.DocumentationWriter\n",
    "        else: \n",
    "            return END\n",
    "\n",
    "    def planner_llm(self, state):\n",
    "        print(\"*** planner_llm ***\")\n",
    "        messages = state['messages']\n",
    "        response = self.model.invoke(messages)\n",
    "        self.llmResponse = [response]\n",
    "    \n",
    "        return {\"messages\": self.llmResponse}\n",
    "\n",
    "    def code_writer_llm(self, state):\n",
    "        print(\"*** Code Writer ***\")\n",
    "        messages = state['messages']\n",
    "        response = self.model.invoke(messages)\n",
    "        self.llmResponse = [response]\n",
    "        return {\"code\": self.llmResponse}\n",
    "    \n",
    "    def documentation_writer_llm(self, input=''):\n",
    "        print(\"*** Docs Writer ***\")\n",
    "        system = \"You are a documentation writer. you read the code and write description above so llm can read it and understand the code responsibility. use tools to read file and write description\"\n",
    "        user = read_file(self.fileName())\n",
    "        messages = [SystemMessage(content=system), HumanMessage(content=user)]\n",
    "        response = self.model.invoke(systemPrompt=system, userPrompt=user)\n",
    "        self.llmResponse = response\n",
    "        return {\"docs\": AIMessage(content=response)}\n",
    "\n",
    "    def draw_graph(self):\n",
    "        return Image(self.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract Agent and Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from LLMs.Askable import Askable\n",
    "from LLMs.Updatable import Updatable\n",
    "from LLMs.WriteReadable import WriteReadable\n",
    "from LLMs.Testable import Testable\n",
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image\n",
    "from prompts.roles import basic_solid_developer\n",
    "\n",
    "class AbstractAgent(ReGraph, Updatable, Askable, WriteReadable, Testable):\n",
    "  def __init__(self, model, modelName = 'gpt-4o-mini'):\n",
    "    self.model = model\n",
    "    self.modelName = modelName\n",
    "    self.llmResponse = None\n",
    "    self.memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "    if self.tools:\n",
    "      self.model = self.model.bind_tools(self.tools)\n",
    "    self.graph = self.make_graph()\n",
    "    self.enforce_subclasses()\n",
    "    \n",
    "  def enforce_subclasses(self):\n",
    "    self.tools\n",
    "    self.prompt()\n",
    "    self.systemPrompt()\n",
    "    self.fileName()\n",
    "  \n",
    "  def invoke(self):\n",
    "    system = f\"\"\"\n",
    "    You are a helpful assistant, your task is to help complete these tasks, using the tools you have:\n",
    "            1. Generate code for the user request.\n",
    "            2. Write the interfaces in another file. \n",
    "            2. Import the needed interface, and write the rest of the code in {self.fileName()}.\n",
    "            3. Generate code description and write it above the code in {self.fileName()}\n",
    "    \"\"\" + self.systemPrompt()\n",
    "    messages = [SystemMessage(content=self.systemPrompt()), HumanMessage(content=self.prompt())]\n",
    "    self.llmResponse = self.start(messages)\n",
    "    print(\"--- done ---\", self.llmResponse)\n",
    "  \n",
    "  def update(self, update):\n",
    "    self.llmResponse = self.update_llm(update, self.systemPrompt(), self.read())\n",
    "    return self.llmResponse\n",
    "  \n",
    "  def test(self, test):\n",
    "    self.llmResponse = self.test_llm(test, self.read(), self.fileName())\n",
    "    return self.llmResponse\n",
    "  \n",
    "  def ask(self, question):\n",
    "    self.llmResponse = self.ask_llm(self.read(), question)\n",
    "    return self.content\n",
    "\n",
    "  def write(self):\n",
    "    self.write_code(self.fileName(), self.content)\n",
    "\n",
    "  def read(self):\n",
    "     return self.read_code(self.fileName())\n",
    "\n",
    "  @property\n",
    "  def content(self):\n",
    "    return self.llmResponse['messages'][-1].content\n",
    "  \n",
    "  # --- api ----\n",
    "  \n",
    "  @abstractmethod\n",
    "  def get_graph(self):\n",
    "    return self.graph\n",
    "\n",
    "  @property\n",
    "  @abstractmethod\n",
    "  def role(self):\n",
    "    return basic_solid_developer()\n",
    "\n",
    "  @property\n",
    "  @abstractmethod\n",
    "  def language(self):\n",
    "    return 'Typescript'\n",
    "\n",
    "  @abstractmethod\n",
    "  def fileName(self):\n",
    "    raise NotImplementedError(\"Subclasses must implement fileName method\")\n",
    "\n",
    "  @abstractmethod\n",
    "  def prompt(self):\n",
    "    raise NotImplementedError(\"Subclasses must implement prompt method\")\n",
    "\n",
    "  @abstractmethod\n",
    "  def systemPrompt(self):\n",
    "    raise NotImplementedError(\"Subclasses must implement systemPrompt method\")\n",
    "\n",
    "  @property\n",
    "  @abstractmethod\n",
    "  def tools(self):\n",
    "    raise NotImplementedError(\"Subclasses must implement tools property\")\n",
    "\n",
    "  @abstractmethod\n",
    "  def add_nodes(self):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def add_edges(self):\n",
    "    pass\n",
    "\n",
    "\n",
    "from prompts.system import solid_dev_prompt\n",
    "from prompts.roles import client_role\n",
    "from utils.files import read_files_in_directory, write_file_tool\n",
    "\n",
    "class ClientAgent(AbstractAgent):\n",
    "\n",
    "  tools = [read_files_in_directory_tool, write_file_tool]\n",
    "\n",
    "  def role(self):\n",
    "    return client_role()\n",
    "\n",
    "  def systemPrompt(self):\n",
    "    return solid_dev_prompt(self.role(), self.language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: [SystemMessage(content='\\n    *** Think step-by-step ***    \\n\\n    # Good interface is..\\n      When you write interfaces, come to this part for instructions.\\n      1. interfaces name starts always with capital I. For example IDuck, IPerson.\\n      2. Do not use types comes from other packages, create your own type.\\n\\n    # Your role in life is..\\n      \\nYou are senior software engineer creates tools or Clients to use in another app services.\\n\\n    You must think like you have just read the \"clean-code\" book by the gang of four.\\n    - Create Interfaces for types uses in the methods instead of using object, any or unknown.\\n    - Provide only the raw code without any markdown formatting, backticks, or language specifications. \\n    - The code should start directly with the first line of {self.language} and end with the last line of code.\\n    - Make sure the code is syntactically correct and can be compiled without errors.\\n    - You must avoid using concrete implementations\\' types, since it\\'s not generic and cannot be later reuse.\\n    - Create your own types instead of using imported from out-side packages.\\n\\n\\n    # Good coding instructions are..\\n      - Always export the classes AND interface you created, do not export default.\\n      - Create an interface for any class you creating.\\n      - Create the types you need for the interface methods.\\n      - Make sure to import packages - if you are using one.\\n      - Use Typescript as the only programming language.\\n      - Good software engineer, do not repeat themselves - so do not repeat your code.\\n\\n    # Coding architecture is base on..\\n      You must obey the SOLID principles:\\n      1. Single-responsibility principle: \"There should never be more than one reason for a class to change.\"In other words, every class should have only one responsibility.\\n      2. Open–closed principle: \"Software entities ... should be open for extension, but closed for modification.\"\\n      3. Liskov substitution principle: \"Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.\\n      4. Interface segregation principle: \"Clients should not be forced to depend upon interfaces that they do not use.\"\\n      5. Dependency inversion principle: \"Depend upon abstractions, not concretes.\"\\n\\n      Note: Constructors are always gets interfaces and never implementations.\\n    '), HumanMessage(content='Create a generic http client class with method for get post delete using fetch. and put all generated files under ./HttpClient')]\n",
      "*** planner_llm ***\n",
      "*** should continue *** content='' additional_kwargs={'tool_calls': [{'id': 'call_FTFNnrAiftP7i6CwQQ54T06B', 'function': {'arguments': '{\"input\": \"export interface IHttpClientOptions {\\\\n    headers?: Record<string, string>;\\\\n    body?: any;\\\\n    params?: Record<string, string | number>;\\\\n}\", \"file_name\": \"./HttpClient/IHttpClientOptions.ts\"}', 'name': 'write_file_tool'}, 'type': 'function'}, {'id': 'call_tP4OCqVjcLyFXlGUnGXVtMPx', 'function': {'arguments': '{\"input\": \"export interface IHttpClientResponse<T> {\\\\n    data: T;\\\\n    status: number;\\\\n    statusText: string;\\\\n}\", \"file_name\": \"./HttpClient/IHttpClientResponse.ts\"}', 'name': 'write_file_tool'}, 'type': 'function'}, {'id': 'call_XBjxRYONqWZuXKyOZhXlJnJ5', 'function': {'arguments': '{\"input\": \"export interface IHttpClient {\\\\n    get<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>>;\\\\n    post<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>>;\\\\n    delete<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>>;\\\\n}\", \"file_name\": \"./HttpClient/IHttpClient.ts\"}', 'name': 'write_file_tool'}, 'type': 'function'}, {'id': 'call_ANMyBvScHqMB4lYoDHK2RUvO', 'function': {'arguments': '{\"input\": \"import { IHttpClient } from \\'./IHttpClient\\';\\\\nimport { IHttpClientOptions } from \\'./IHttpClientOptions\\';\\\\nimport { IHttpClientResponse } from \\'./IHttpClientResponse\\';\\\\n\\\\nexport class HttpClient implements IHttpClient {\\\\n    async get<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>> {\\\\n        const response = await fetch(url + this.createQueryString(options?.params), {\\\\n            method: \\'GET\\',\\\\n            headers: options?.headers,\\\\n        });\\\\n        const data = await response.json();\\\\n        return { data, status: response.status, statusText: response.statusText };\\\\n    }\\\\n\\\\n    async post<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>> {\\\\n        const response = await fetch(url, {\\\\n            method: \\'POST\\',\\\\n            headers: { \\'Content-Type\\': \\'application/json\\', ...options?.headers },\\\\n            body: JSON.stringify(options?.body),\\\\n        });\\\\n        const data = await response.json();\\\\n        return { data, status: response.status, statusText: response.statusText };\\\\n    }\\\\n\\\\n    async delete<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>> {\\\\n        const response = await fetch(url, {\\\\n            method: \\'DELETE\\',\\\\n            headers: options?.headers,\\\\n        });\\\\n        const data = await response.json();\\\\n        return { data, status: response.status, statusText: response.statusText };\\\\n    }\\\\n\\\\n    private createQueryString(params?: Record<string, string | number>): string {\\\\n        if (!params) return \\'\\';\\\\n        const queryString = new URLSearchParams(params).toString();\\\\n        return `?${queryString}`;\\\\n    }\\\\n}\", \"file_name\": \"./HttpClient/HttpClient.ts\"}', 'name': 'write_file_tool'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 659, 'prompt_tokens': 2882, 'total_tokens': 3541}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-4942d8b3-d420-477a-9ddc-f9fffc56f678-0' tool_calls=[{'name': 'write_file_tool', 'args': {'input': 'export interface IHttpClientOptions {\\n    headers?: Record<string, string>;\\n    body?: any;\\n    params?: Record<string, string | number>;\\n}', 'file_name': './HttpClient/IHttpClientOptions.ts'}, 'id': 'call_FTFNnrAiftP7i6CwQQ54T06B', 'type': 'tool_call'}, {'name': 'write_file_tool', 'args': {'input': 'export interface IHttpClientResponse<T> {\\n    data: T;\\n    status: number;\\n    statusText: string;\\n}', 'file_name': './HttpClient/IHttpClientResponse.ts'}, 'id': 'call_tP4OCqVjcLyFXlGUnGXVtMPx', 'type': 'tool_call'}, {'name': 'write_file_tool', 'args': {'input': 'export interface IHttpClient {\\n    get<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>>;\\n    post<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>>;\\n    delete<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>>;\\n}', 'file_name': './HttpClient/IHttpClient.ts'}, 'id': 'call_XBjxRYONqWZuXKyOZhXlJnJ5', 'type': 'tool_call'}, {'name': 'write_file_tool', 'args': {'input': \"import { IHttpClient } from './IHttpClient';\\nimport { IHttpClientOptions } from './IHttpClientOptions';\\nimport { IHttpClientResponse } from './IHttpClientResponse';\\n\\nexport class HttpClient implements IHttpClient {\\n    async get<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>> {\\n        const response = await fetch(url + this.createQueryString(options?.params), {\\n            method: 'GET',\\n            headers: options?.headers,\\n        });\\n        const data = await response.json();\\n        return { data, status: response.status, statusText: response.statusText };\\n    }\\n\\n    async post<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>> {\\n        const response = await fetch(url, {\\n            method: 'POST',\\n            headers: { 'Content-Type': 'application/json', ...options?.headers },\\n            body: JSON.stringify(options?.body),\\n        });\\n        const data = await response.json();\\n        return { data, status: response.status, statusText: response.statusText };\\n    }\\n\\n    async delete<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>> {\\n        const response = await fetch(url, {\\n            method: 'DELETE',\\n            headers: options?.headers,\\n        });\\n        const data = await response.json();\\n        return { data, status: response.status, statusText: response.statusText };\\n    }\\n\\n    private createQueryString(params?: Record<string, string | number>): string {\\n        if (!params) return '';\\n        const queryString = new URLSearchParams(params).toString();\\n        return `?${queryString}`;\\n    }\\n}\", 'file_name': './HttpClient/HttpClient.ts'}, 'id': 'call_ANMyBvScHqMB4lYoDHK2RUvO', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2882, 'output_tokens': 659, 'total_tokens': 3541}\n",
      "** tool-use: write_file\n",
      "** tool-use: write_file\n",
      "** tool-use: write_file\n",
      "** tool-use: write_file\n",
      "*** planner_llm ***\n",
      "*** should continue *** content=\"export interface IHttpClientOptions {\\n    headers?: Record<string, string>;\\n    body?: any;\\n    params?: Record<string, string | number>;\\n}\\n\\nexport interface IHttpClientResponse<T> {\\n    data: T;\\n    status: number;\\n    statusText: string;\\n}\\n\\nexport interface IHttpClient {\\n    get<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>>;\\n    post<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>>;\\n    delete<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>>;\\n}\\n\\nimport { IHttpClient } from './IHttpClient';\\nimport { IHttpClientOptions } from './IHttpClientOptions';\\nimport { IHttpClientResponse } from './IHttpClientResponse';\\n\\nexport class HttpClient implements IHttpClient {\\n    async get<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>> {\\n        const response = await fetch(url + this.createQueryString(options?.params), {\\n            method: 'GET',\\n            headers: options?.headers,\\n        });\\n        const data = await response.json();\\n        return { data, status: response.status, statusText: response.statusText };\\n    }\\n\\n    async post<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>> {\\n        const response = await fetch(url, {\\n            method: 'POST',\\n            headers: { 'Content-Type': 'application/json', ...options?.headers },\\n            body: JSON.stringify(options?.body),\\n        });\\n        const data = await response.json();\\n        return { data, status: response.status, statusText: response.statusText };\\n    }\\n\\n    async delete<T>(url: string, options?: IHttpClientOptions): Promise<IHttpClientResponse<T>> {\\n        const response = await fetch(url, {\\n            method: 'DELETE',\\n            headers: options?.headers,\\n        });\\n        const data = await response.json();\\n        return { data, status: response.status, statusText: response.statusText };\\n    }\\n\\n    private createQueryString(params?: Record<string, string | number>): string {\\n        if (!params) return '';\\n        const queryString = new URLSearchParams(params).toString();\\n        return `?${queryString}`;\\n    }\\n}\" response_metadata={'token_usage': {'completion_tokens': 467, 'prompt_tokens': 3560, 'total_tokens': 4027}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'stop', 'logprobs': None} id='run-f2ed8958-d572-451c-813c-0a471e11e3ca-0' usage_metadata={'input_tokens': 3560, 'output_tokens': 467, 'total_tokens': 4027}\n",
      "*** Code Writer ***\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'message' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m httpClient \u001b[38;5;241m=\u001b[39m HttpClient(llm)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# httpClient.draw_graph()\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mhttpClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 38\u001b[0m, in \u001b[0;36mAbstractAgent.invoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m system \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124mYou are a helpful assistant, your task is to help complete these tasks, using the tools you have:\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m        1. Generate code for the user request.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124m        3. Generate code description and write it above the code in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfileName()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystemPrompt()\n\u001b[1;32m     37\u001b[0m messages \u001b[38;5;241m=\u001b[39m [SystemMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystemPrompt()), HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt())]\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllmResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- done ---\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllmResponse)\n",
      "Cell \u001b[0;32mIn[41], line 47\u001b[0m, in \u001b[0;36mReGraph.start\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart:\u001b[39m\u001b[38;5;124m\"\u001b[39m, messages)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthread_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/github/ai/reimagined/.conda/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1668\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1667\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1668\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/Development/github/ai/reimagined/.conda/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1111\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m-> 1111\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/Development/github/ai/reimagined/.conda/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1758\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[1;32m   1756\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1757\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1761\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1762\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1763\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/github/ai/reimagined/.conda/lib/python3.11/site-packages/langgraph/pregel/executor.py:43\u001b[0m, in \u001b[0;36mBackgroundExecutor.<locals>.done\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/github/ai/reimagined/.conda/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/Development/github/ai/reimagined/.conda/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/github/ai/reimagined/.conda/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Development/github/ai/reimagined/.conda/lib/python3.11/site-packages/langgraph/pregel/retry.py:25\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     23\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/github/ai/reimagined/.conda/lib/python3.11/site-packages/langchain_core/runnables/base.py:2871\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2867\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2868\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2869\u001b[0m )\n\u001b[1;32m   2870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2871\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2873\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Development/github/ai/reimagined/.conda/lib/python3.11/site-packages/langgraph/utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[41], line 76\u001b[0m, in \u001b[0;36mReGraph.code_writer_llm\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Code Writer ***\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m messages \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 76\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minvoke(\u001b[43mmessage\u001b[49m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllmResponse \u001b[38;5;241m=\u001b[39m [response]\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllmResponse}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'message' is not defined"
     ]
    }
   ],
   "source": [
    "class HttpClient(ClientAgent):\n",
    "    def fileName(self):\n",
    "        return \"HttpClient.ts\"\n",
    "    \n",
    "    def prompt(self):\n",
    "        return \"Create a generic http client class with method for get post delete using fetch. and put all generated files under ./HttpClient\"\n",
    "\n",
    "httpClient = HttpClient(llm)\n",
    "# httpClient.draw_graph()\n",
    "httpClient.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tools:\n",
    "# 1. Read the architecture and files tool\n",
    "# 2. Read a webpage tool\n",
    "# 3. Read specific file tool - done\n",
    "# 4. Write a file tool, to write interfaces as a separate file - done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start using Langgraph state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLMs.Invokable import Invokable\n",
    "from utils.files import write_file_tool, write_docs_tool, read_file_tool, read_files_in_directory_tool\n",
    "from langchain_core.tools import tool\n",
    "import random\n",
    "\n",
    "@tool\n",
    "def location_map(input=''):\n",
    "    \"\"\"Helpful when need to determine the current location\"\"\"\n",
    "    print(\"location map called\")\n",
    "    return \"Petach Tikva\"\n",
    "\n",
    "\n",
    "thread_id = 4\n",
    "\n",
    "class Agents:\n",
    "    Planner = \"planner\"\n",
    "    LocationChecker = 'location_checker'\n",
    "    Tools = 'tools'\n",
    "\n",
    "class ReGraph1(Invokable):\n",
    "    def __init__(self, model):\n",
    "        self.modelName = 'gpt-4o-mini'\n",
    "        self.memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "        self.make_graph()\n",
    "\n",
    "    def tools(self):\n",
    "        return [read_files_in_directory_tool, write_file_tool]\n",
    "\n",
    "    def make_graph(self):\n",
    "        self.model = ChatOpenAI(model='gpt-4o-mini').bind_tools(self.tools())\n",
    "        \n",
    "        tool_node = ToolNode(self.tools())\n",
    "        \n",
    "        self.graph = StateGraph(AgentState)\n",
    "        self.graph.add_node(Agents.Planner, self.planner_llm)\n",
    "        self.graph.add_node(Agents.Tools, tool_node)\n",
    "        self.graph.add_conditional_edges(Agents.Planner, self.should_continue)\n",
    "        self.graph.add_edge(Agents.Tools, Agents.Planner)\n",
    "        \n",
    "        self.graph.set_entry_point(Agents.Planner)\n",
    "\n",
    "        self.graph = self.graph.compile(memory)\n",
    "        return self.graph\n",
    "    \n",
    "    def start(self, messages):\n",
    "        print(\"start:\", messages)\n",
    "        return self.graph.invoke(\n",
    "            {\"messages\": messages},\n",
    "            config={\"configurable\": {\"thread_id\": random.randint(1, 100)}}\n",
    "        )\n",
    "\n",
    "    def should_continue(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        last_message = messages[-1]\n",
    "        print(\"*** should continue ***\", last_message)\n",
    "        if last_message.tool_calls:\n",
    "            return Agents.Tools\n",
    "        return END\n",
    "\n",
    "    def planner_llm(self, state):\n",
    "        print(\"*** planner_llm ***\", state['messages'])\n",
    "        messages = state['messages']\n",
    "        response = self.model.invoke(messages)\n",
    "        print('response', response)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "    def draw_graph(self):\n",
    "        return Image(self.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ReGraph1(model=llm)\n",
    "messages = [\n",
    "    SystemMessage(content='You are a helpful import code assistant. use the tools to scan folders and read files'), \n",
    "    HumanMessage(content=\"\"\"\n",
    "        1. Find the interface code I need to implement to create http client `./test`. \n",
    "        2. import the interface and write a typescript implementation of it\n",
    "        3. put the code in another file called http-client.ts\n",
    "    \"\"\")\n",
    "]\n",
    "graph.start(messages)\n",
    "# graph2.draw_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: [SystemMessage(content='You are a helpful import code assistant. use the tools to scan folders and read files'), HumanMessage(content='\\n        1. Find the code that can calculate the distance from me to the end of the room under folder `./test`. \\n        2. write typescript code using the distance function to measure the input 53.\\n        3. put the code in another file called 53.ts\\n    ')]\n",
      "*** planner_llm *** [SystemMessage(content='You are a helpful import code assistant. use the tools to scan folders and read files'), HumanMessage(content='\\n        1. Find the code that can calculate the distance from me to the end of the room under folder `./test`. \\n        2. write typescript code using the distance function to measure the input 53.\\n        3. put the code in another file called 53.ts\\n    ')]\n",
      "response content='' additional_kwargs={'tool_calls': [{'id': 'call_oBRm9YWQzDrhnwCvcb5NfOLl', 'function': {'arguments': '{\"input\":\"./test\"}', 'name': 'read_files_in_directory_tool'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 210, 'total_tokens': 228}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-8578fcaf-63b6-4555-bf4d-46b706de069f-0' tool_calls=[{'name': 'read_files_in_directory_tool', 'args': {'input': './test'}, 'id': 'call_oBRm9YWQzDrhnwCvcb5NfOLl', 'type': 'tool_call'}] usage_metadata={'input_tokens': 210, 'output_tokens': 18, 'total_tokens': 228}\n",
      "*** should continue *** content='' additional_kwargs={'tool_calls': [{'id': 'call_oBRm9YWQzDrhnwCvcb5NfOLl', 'function': {'arguments': '{\"input\":\"./test\"}', 'name': 'read_files_in_directory_tool'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 210, 'total_tokens': 228}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-8578fcaf-63b6-4555-bf4d-46b706de069f-0' tool_calls=[{'name': 'read_files_in_directory_tool', 'args': {'input': './test'}, 'id': 'call_oBRm9YWQzDrhnwCvcb5NfOLl', 'type': 'tool_call'}] usage_metadata={'input_tokens': 210, 'output_tokens': 18, 'total_tokens': 228}\n",
      "** tool-use: read_files_in_directory\n",
      "Reading from: ./test/calculator.ts\n",
      "Reading from: ./test/IHttpClient.ts\n",
      "*** planner_llm *** [SystemMessage(content='You are a helpful import code assistant. use the tools to scan folders and read files'), HumanMessage(content='\\n        1. Find the code that can calculate the distance from me to the end of the room under folder `./test`. \\n        2. write typescript code using the distance function to measure the input 53.\\n        3. put the code in another file called 53.ts\\n    '), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_oBRm9YWQzDrhnwCvcb5NfOLl', 'function': {'arguments': '{\"input\":\"./test\"}', 'name': 'read_files_in_directory_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 210, 'total_tokens': 228}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8578fcaf-63b6-4555-bf4d-46b706de069f-0', tool_calls=[{'name': 'read_files_in_directory_tool', 'args': {'input': './test'}, 'id': 'call_oBRm9YWQzDrhnwCvcb5NfOLl', 'type': 'tool_call'}], usage_metadata={'input_tokens': 210, 'output_tokens': 18, 'total_tokens': 228}), ToolMessage(content='export class Test {\\n    calculate_distance(input: number) {\\n        return input * 2\\n    }\\n}export interface IHttpClient<T, Y> {\\n    get<T>(url: string): Promise<Y>\\n}', name='read_files_in_directory_tool', tool_call_id='call_oBRm9YWQzDrhnwCvcb5NfOLl')]\n",
      "response content='' additional_kwargs={'tool_calls': [{'id': 'call_IMnRFzRExtpQncli9BC2KiVU', 'function': {'arguments': '{\"input\":\"import { Test } from \\'./test\\';\\\\n\\\\nconst test = new Test();\\\\nconst distance = test.calculate_distance(53);\\\\nconsole.log(`The distance is: ${distance}`);\",\"file_name\":\"53.ts\"}', 'name': 'write_file_tool'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 283, 'total_tokens': 343}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-ca20e2e0-74e4-4cda-9c0f-c08abc2b1502-0' tool_calls=[{'name': 'write_file_tool', 'args': {'input': \"import { Test } from './test';\\n\\nconst test = new Test();\\nconst distance = test.calculate_distance(53);\\nconsole.log(`The distance is: ${distance}`);\", 'file_name': '53.ts'}, 'id': 'call_IMnRFzRExtpQncli9BC2KiVU', 'type': 'tool_call'}] usage_metadata={'input_tokens': 283, 'output_tokens': 60, 'total_tokens': 343}\n",
      "*** should continue *** content='' additional_kwargs={'tool_calls': [{'id': 'call_IMnRFzRExtpQncli9BC2KiVU', 'function': {'arguments': '{\"input\":\"import { Test } from \\'./test\\';\\\\n\\\\nconst test = new Test();\\\\nconst distance = test.calculate_distance(53);\\\\nconsole.log(`The distance is: ${distance}`);\",\"file_name\":\"53.ts\"}', 'name': 'write_file_tool'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 283, 'total_tokens': 343}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-ca20e2e0-74e4-4cda-9c0f-c08abc2b1502-0' tool_calls=[{'name': 'write_file_tool', 'args': {'input': \"import { Test } from './test';\\n\\nconst test = new Test();\\nconst distance = test.calculate_distance(53);\\nconsole.log(`The distance is: ${distance}`);\", 'file_name': '53.ts'}, 'id': 'call_IMnRFzRExtpQncli9BC2KiVU', 'type': 'tool_call'}] usage_metadata={'input_tokens': 283, 'output_tokens': 60, 'total_tokens': 343}\n",
      "** tool-use: write_file\n",
      "*** planner_llm *** [SystemMessage(content='You are a helpful import code assistant. use the tools to scan folders and read files'), HumanMessage(content='\\n        1. Find the code that can calculate the distance from me to the end of the room under folder `./test`. \\n        2. write typescript code using the distance function to measure the input 53.\\n        3. put the code in another file called 53.ts\\n    '), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_oBRm9YWQzDrhnwCvcb5NfOLl', 'function': {'arguments': '{\"input\":\"./test\"}', 'name': 'read_files_in_directory_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 210, 'total_tokens': 228}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8578fcaf-63b6-4555-bf4d-46b706de069f-0', tool_calls=[{'name': 'read_files_in_directory_tool', 'args': {'input': './test'}, 'id': 'call_oBRm9YWQzDrhnwCvcb5NfOLl', 'type': 'tool_call'}], usage_metadata={'input_tokens': 210, 'output_tokens': 18, 'total_tokens': 228}), ToolMessage(content='export class Test {\\n    calculate_distance(input: number) {\\n        return input * 2\\n    }\\n}export interface IHttpClient<T, Y> {\\n    get<T>(url: string): Promise<Y>\\n}', name='read_files_in_directory_tool', tool_call_id='call_oBRm9YWQzDrhnwCvcb5NfOLl'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IMnRFzRExtpQncli9BC2KiVU', 'function': {'arguments': '{\"input\":\"import { Test } from \\'./test\\';\\\\n\\\\nconst test = new Test();\\\\nconst distance = test.calculate_distance(53);\\\\nconsole.log(`The distance is: ${distance}`);\",\"file_name\":\"53.ts\"}', 'name': 'write_file_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 283, 'total_tokens': 343}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ca20e2e0-74e4-4cda-9c0f-c08abc2b1502-0', tool_calls=[{'name': 'write_file_tool', 'args': {'input': \"import { Test } from './test';\\n\\nconst test = new Test();\\nconst distance = test.calculate_distance(53);\\nconsole.log(`The distance is: ${distance}`);\", 'file_name': '53.ts'}, 'id': 'call_IMnRFzRExtpQncli9BC2KiVU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 283, 'output_tokens': 60, 'total_tokens': 343}), ToolMessage(content='null', name='write_file_tool', tool_call_id='call_IMnRFzRExtpQncli9BC2KiVU')]\n",
      "response content=\"I found the distance calculation code in the `./test` folder, and I created a TypeScript file called `53.ts` that uses the distance function to measure the input 53. The content of the `53.ts` file is as follows:\\n\\n```typescript\\nimport { Test } from './test';\\n\\nconst test = new Test();\\nconst distance = test.calculate_distance(53);\\nconsole.log(`The distance is: ${distance}`);\\n```\" response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 353, 'total_tokens': 442}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'stop', 'logprobs': None} id='run-1f2270d1-28c7-4b88-ad8a-4f42d42e9f53-0' usage_metadata={'input_tokens': 353, 'output_tokens': 89, 'total_tokens': 442}\n",
      "*** should continue *** content=\"I found the distance calculation code in the `./test` folder, and I created a TypeScript file called `53.ts` that uses the distance function to measure the input 53. The content of the `53.ts` file is as follows:\\n\\n```typescript\\nimport { Test } from './test';\\n\\nconst test = new Test();\\nconst distance = test.calculate_distance(53);\\nconsole.log(`The distance is: ${distance}`);\\n```\" response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 353, 'total_tokens': 442}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'stop', 'logprobs': None} id='run-1f2270d1-28c7-4b88-ad8a-4f42d42e9f53-0' usage_metadata={'input_tokens': 353, 'output_tokens': 89, 'total_tokens': 442}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content='You are a helpful import code assistant. use the tools to scan folders and read files'),\n",
       "  HumanMessage(content='\\n        1. Find the code that can calculate the distance from me to the end of the room under folder `./test`. \\n        2. write typescript code using the distance function to measure the input 53.\\n        3. put the code in another file called 53.ts\\n    '),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_oBRm9YWQzDrhnwCvcb5NfOLl', 'function': {'arguments': '{\"input\":\"./test\"}', 'name': 'read_files_in_directory_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 210, 'total_tokens': 228}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8578fcaf-63b6-4555-bf4d-46b706de069f-0', tool_calls=[{'name': 'read_files_in_directory_tool', 'args': {'input': './test'}, 'id': 'call_oBRm9YWQzDrhnwCvcb5NfOLl', 'type': 'tool_call'}], usage_metadata={'input_tokens': 210, 'output_tokens': 18, 'total_tokens': 228}),\n",
       "  ToolMessage(content='export class Test {\\n    calculate_distance(input: number) {\\n        return input * 2\\n    }\\n}export interface IHttpClient<T, Y> {\\n    get<T>(url: string): Promise<Y>\\n}', name='read_files_in_directory_tool', tool_call_id='call_oBRm9YWQzDrhnwCvcb5NfOLl'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IMnRFzRExtpQncli9BC2KiVU', 'function': {'arguments': '{\"input\":\"import { Test } from \\'./test\\';\\\\n\\\\nconst test = new Test();\\\\nconst distance = test.calculate_distance(53);\\\\nconsole.log(`The distance is: ${distance}`);\",\"file_name\":\"53.ts\"}', 'name': 'write_file_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 283, 'total_tokens': 343}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ca20e2e0-74e4-4cda-9c0f-c08abc2b1502-0', tool_calls=[{'name': 'write_file_tool', 'args': {'input': \"import { Test } from './test';\\n\\nconst test = new Test();\\nconst distance = test.calculate_distance(53);\\nconsole.log(`The distance is: ${distance}`);\", 'file_name': '53.ts'}, 'id': 'call_IMnRFzRExtpQncli9BC2KiVU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 283, 'output_tokens': 60, 'total_tokens': 343}),\n",
       "  ToolMessage(content='null', name='write_file_tool', tool_call_id='call_IMnRFzRExtpQncli9BC2KiVU'),\n",
       "  AIMessage(content=\"I found the distance calculation code in the `./test` folder, and I created a TypeScript file called `53.ts` that uses the distance function to measure the input 53. The content of the `53.ts` file is as follows:\\n\\n```typescript\\nimport { Test } from './test';\\n\\nconst test = new Test();\\nconst distance = test.calculate_distance(53);\\nconsole.log(`The distance is: ${distance}`);\\n```\", response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 353, 'total_tokens': 442}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0f03d4f0ee', 'finish_reason': 'stop', 'logprobs': None}, id='run-1f2270d1-28c7-4b88-ad8a-4f42d42e9f53-0', usage_metadata={'input_tokens': 353, 'output_tokens': 89, 'total_tokens': 442})]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = ReGraph1(model=llm)\n",
    "messages = [\n",
    "    SystemMessage(content='You are a helpful import code assistant. use the tools to scan folders and read files'), \n",
    "    HumanMessage(content=\"\"\"\n",
    "        1. Find the code that can calculate the distance from me to the end of the room under folder `./test`. \n",
    "        2. write typescript code using the distance function to measure the input 53.\n",
    "        3. put the code in another file called 53.ts\n",
    "    \"\"\")\n",
    "]\n",
    "graph.start(messages)\n",
    "# graph2.draw_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "modelName = \"gpt-4o-mini\"\n",
    "model = ChatOpenAI(model=modelName)\n",
    "\n",
    "\n",
    "PROMPT_PLANNER = \"\"\"\n",
    "You are a helpful assistant.\n",
    "The main task is to rate how likely a content is a spam or not.\n",
    "\n",
    "The tasks are:\n",
    "1. Read the email text from the .txt file\n",
    "2. Rate how likely the email is a spam or not.\n",
    "3. Write the likelihood percentage in the email txt file.\n",
    "\n",
    "Use the tools you have to complete the tasks.\n",
    "\"\"\"\n",
    "\n",
    "@tool\n",
    "def read_file(file_name):\n",
    "    \"\"\"Useful to when need to read single file content\"\"\"\n",
    "    print(\"**** reading file ***\", file_name)\n",
    "    with open(file_name, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "@tool\n",
    "def write_file(input=\"\", file_name=\"\"):\n",
    "    \"\"\"Useful to when need to write content into a file, get's two params, 1. input: the content you want to insert to the file. 2. file_name: the file name you want to edit.\"\"\"\n",
    "    content = read_file(file_name)\n",
    "    print(\"enter write file\", content)\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.writelines([f\"// Description: {input}\", f\"\\n{content}\\n\"])\n",
    "\n",
    "@tool\n",
    "def read_files_in_directory(input=\"\"):\n",
    "    \"\"\"Useful to when need to read folder files content\"\"\"\n",
    "    for root, dirs, files in os.walk('test_folder'):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                print(f\"Reading from: {file_path}\")\n",
    "                print(content)\n",
    "    \n",
    "def should_continue(state: AgentState):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    print(\"should continue\", last_message)\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "def call_planner(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def agent_read_and_write(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "    \n",
    "\n",
    "tools = [read_file, write_file]\n",
    "tool_node = ToolNode(tools)\n",
    "model = ChatOpenAI(model=modelName).bind_tools(tools)\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"planner\", call_planner)\n",
    "\n",
    "graph.add_node(\"tools\", tool_node)\n",
    "graph.add_conditional_edges(\"planner\", should_continue)\n",
    "graph.add_edge('tools', 'planner')\n",
    "\n",
    "graph.set_entry_point('planner')\n",
    "\n",
    "graph = graph.compile(memory)\n",
    "\n",
    "final_state = graph.invoke(\n",
    "    {\"messages\": [SystemMessage(content=PROMPT_PLANNER)]},\n",
    "    config={\"configurable\": {\"thread_id\": 27}}\n",
    ")\n",
    "\n",
    "final_state['messages'][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
